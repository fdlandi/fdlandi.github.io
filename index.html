<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>federicolandi.info</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Federico Landi</name>
              </p>
		<p>
		I am currently working as a Deep Learning Engineer at Huawei Technologies, in the Amsterdam Research Center.
		</p>
		<p>
                I got my Ph.D. at <a href="http://imagelab.ing.unimore.it/imagelab/index.asp">AimageLab</a> at the University of Modena and Reggio Emilia, Italy, under the supervision of <a href="http://imagelab.ing.unimore.it/imagelab/person.asp?idpersona=1">Prof. Rita Cucchiara</a>. DUring my Ph.D, my research focused on the fascinating topic of Embodied AI, at the intersection of Computer Vision, Deep Learning, and Robotics.
              </p>
		<p>
		As part of my Master Thesis, I was a visiting student at <a href="https://www.uva.nl/">University of Amsterdam (UVA)</a> where I worked under the supervision of <a href="https://www.ceessnoek.info/">Prof. Cees Snoek</a>.
              </p>
 
              <p style="text-align:center">
                <a href="mailto:federicolandi94@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/FedericoLandi-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.it/citations?user=YYvdzXYAAAAJ&hl=it">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/fdlandi">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/federico-landi-75712718a/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
		<img style="width:100%;max-width:100%;border-radius:50%;" alt="profile photo" src="images/FedericoLandi.jpeg" class="hoverZoomLink"/>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
		In the first part of my Ph.D. I tackled the recent task of Vision-and-Language Navigation. More recently, I developed a strong interest in Embodied exploration and navigation, as well as in Recurrent Neural Networks.
              </p>
            </td>
          </tr>
        </tbody></table>
<!-- paper list begins -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="two" id='impact_image'><img src='images/spot.svg'></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Spot the Difference: A Novel Task for Embodied Agents in Changing Environments</papertitle>
              <br>
			<strong>Federico Landi</strong>,
			  <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=121">Roberto Bigazzi</a>,
			  <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=90">Marcella Cornia</a>,	
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=122">Silvia Cascianelli</a>,			  
			  <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=82">Lorenzo Baraldi</a>,
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1">Rita Cucchiara</a>	
              <br>
              <em>ICPR</em>, 2022
		<br>
              <a href="https://arxiv.org/abs/2204.08502" target="_blank">arXiv</a> &nbsp/&nbsp
              <a href="data/spot.bib" target="_blank">bibtex</a> &nbsp/&nbsp
		<a href="https://github.com/aimagelab/spot-the-difference" target="_blank">code</a>
		<br>
		<br>
              <p></p>
              <p>We propose Spot the Difference: a novel task for Embodied AI.</p>
            </td>
        </tr>
        </tbody></table>
<!-- next paper -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="two" id='impact_image'><img src='images/focus_on_impact.svg'></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Focus on Impact: Indoor Exploration with Intrinsic Motivation</papertitle>
              <br>
			  <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=121">Roberto Bigazzi</a>,
              <strong>Federico Landi</strong>,
			  <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=122">Silvia Cascianelli</a>,			  
			  <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=82">Lorenzo Baraldi</a>,
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=90">Marcella Cornia</a>,	
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1">Rita Cucchiara</a>	
              <br>
              <em>RAL/ICRA</em>, 2022
		<br>
              <a href="https://arxiv.org/abs/2109.08521" target="_blank">arXiv</a> &nbsp/&nbsp
              <a href="data/impact.bib" target="_blank">bibtex</a> &nbsp/&nbsp
		<a href="https://github.com/aimagelab/focus-on-impact" target="_blank">code</a>
		<br>
		<br>
              <p></p>
              <p>We devise an impact-based intrinsic reward to train embodied exploration agents in photorealistic indoor environments.</p>
            </td>
        </tr>
        </tbody></table>
<!-- next paper -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="two" id='impact_image'><img src='images/artgallery.svg'></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Embodied Navigation at the Art Gallery</papertitle>
              <br>
			  <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=121">Roberto Bigazzi</a>,
              <strong>Federico Landi</strong>,
			  <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=122">Silvia Cascianelli</a>,			  
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=90">Marcella Cornia</a>,
			  <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=82">Lorenzo Baraldi</a>,	
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1">Rita Cucchiara</a>	
              <br>
              <em>ICIAP</em>, 2021
		<br>
              <a href="https://arxiv.org/abs/2204.09069" target="_blank">arXiv</a> &nbsp/&nbsp
              <a href="data/artgallery.bib" target="_blank">bibtex</a> &nbsp/&nbsp
		<a href="https://github.com/aimagelab/ag3d" target="_blank">code</a>
		<br>
		<br>
              <p></p>
              <p>We build and release a new 3D space displaying a complete art museum, called ArtGallery3D (AG3D).</p>
            </td>
        </tr>
        </tbody></table>
<!-- next paper -->
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="two" id='wmc_image'><img src='images/working_memory_connections.svg'></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
		<a href=https://www.sciencedirect.com/science/article/pii/S0893608021003439>
                <papertitle>Working Memory Connections for LSTM</papertitle> </a>
              <br>
              		<strong>Federico Landi</strong>,		  
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=82">Lorenzo Baraldi</a>,
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=90">Marcella Cornia</a>,	
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1">Rita Cucchiara</a>	
              <br>
              <em>NEUNET</em>, 2021
		<br>
              <a href="https://arxiv.org/abs/2109.00020" target="_blank">arXiv</a> &nbsp/&nbsp
              <a href="data/working_memory.bib" target="_blank">bibtex</a>
		<br>
              <p></p>
              <p>A simple heuristic improvement boosts LSTM performance on a variety of tasks. Our approach shows more stable training dynamics and faster convergence time when compared to vanilla LSTM and peephole LSTM.</p>
            </td>
        </tr>
        </tbody></table>
<!-- next paper -->
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="two" id='pta_image'><img src='images/pta.svg'></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
		<a href=https://www.sciencedirect.com/science/article/pii/S1077314221000990>
                <papertitle>Multimodal Attention Networks for Low-Level Vision-and-Language Navigation</papertitle> </a>
              <br>
              		<strong>Federico Landi</strong>,		  
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=82">Lorenzo Baraldi</a>,
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=90">Marcella Cornia</a>,
			<a href="http://vcg.isti.cnr.it/~corsini/">Massimiliano Corsini</a>,	
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1">Rita Cucchiara</a>	
              <br>
              <em>CVIU</em>, 2021
		<br>
              	<a href="https://arxiv.org/abs/1911.12377" target="_blank">arXiv</a> &nbsp/&nbsp
              	<a href="data/pta.bib" target="_blank">bibtex</a> &nbsp/&nbsp
		<a href="https://github.com/aimagelab/perceive-transform-and-act" target="_blank">code</a>
		<br>
              <p></p>
              <p>The first fully-attentive approach to Vision-and-Language Navigation (VLN). We achieve state-of-art performance on low-level VLN, a setting which is rarely considered in the literature given its additional difficulty.</p>
            </td>
        </tr>
        </tbody></table>
<!-- next paper -->
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="two" id='loconav_image'><img src='images/out_of_the_box.svg'></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Out of the Box: Embodied Navigation in the Real World</papertitle>
              <br>
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=121">Roberto Bigazzi</a>,
			<strong>Federico Landi</strong>,
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=90">Marcella Cornia</a>,
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=122">Silvia Cascianelli</a>,	              	
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=82">Lorenzo Baraldi</a>,
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1">Rita Cucchiara</a>	
              <br>
              <em>CAIP</em>, 2021
		<br>
		<a href="https://arxiv.org/abs/2105.05873" target="_blank">arXiv</a> &nbsp/&nbsp
		<a href="data/out_of_the_box.bib" target="_blank">bibtex</a> &nbsp/&nbsp
		<a href="https://github.com/aimagelab/LoCoNav" target="_blank">code</a> &nbsp/&nbsp
		<a href="data/Out_of_the_Box.pdf" target="_blank">slides</a>
		<br>
              <p></p>
              <p>We detail how to deploy a navigation policy trained on the Habitat simulator on a LoCoBot. Additionally, we study the performance on five different PointGoal navigation episodes in a real-world, challenging setting.</p>
            </td>
        </tr>
        </tbody></table>
<!-- next paper -->
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="two" id='vtongthd_image'><img src='images/vtonGThd.svg'></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Transform, Warp, and Dress: A New Transformation-Guided Model for Virtual Try-On</papertitle>
              <br>
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=126">Matteo Fincato</a>,
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=90">Marcella Cornia</a>,
			<strong>Federico Landi</strong>,
			Fabio Cesari,
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1">Rita Cucchiara</a>	
              <br>
              <em>TOMM</em>, 2021
		<br>
		<a href="data/transform_warp_dress.bib" target="_blank">bibtex</a>
		<br>
              <p></p>
              <p>We present a new dataset of upper-body clothes for virtual try-on with high-resolution images. We also propose a new model for virtual try-on that can generate high-quality images using a three-stage pipeline.</p>
            </td>
        </tr>
        </tbody></table>
<!-- next paper -->
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="two" id='vtongt_image'><img src='images/vtonGT.svg'></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://ieeexplore.ieee.org/document/9412052">
                <papertitle>VITON-GT: An Image-based Virtual Try-On Model with Geometric Transformations</papertitle></a>
              <br>
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=126">Matteo Fincato</a>,
			<strong>Federico Landi</strong>,			
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=90">Marcella Cornia</a>,			
			Fabio Cesari,
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1">Rita Cucchiara</a>		
              <br>
              <em>ICPR</em>, 2020
		<br>
		<a href="https://ieeexplore.ieee.org/document/9412052" target="_blank">paper</a> &nbsp/&nbsp
		<a href="data/vtonGT.bib" target="_blank">bibtex</a> &nbsp/&nbsp
		<a href="data/Viton-GT_Poster.pdf" target="_blank">poster</a> &nbsp/&nbsp
		<a href="data/Viton-GT.pdf" target="_blank">slides</a> &nbsp/&nbsp
		<a href="data/Viton-GT.mp4" target="_blank">presentation (video)</a>
		<br>
              <p></p>
              <p>We propose a new image-based virtual try-on model that can generate high-quality images. We exploit learnable affine and thin-plate spline transformations, combined with a generative network, to create new images of a person wearing different clothes.</p>
            </td>
        </tr>
        </tbody></table>
<!-- next paper -->
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="two" id='explore_and_explain_image'><img src='images/explore_and_explain.svg'></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://ieeexplore.ieee.org/document/9412628">
                <papertitle>Explore and Explain: Self-supervised Navigation and Recounting</papertitle></a>
              <br>
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=121">Roberto Bigazzi</a>,
			<strong>Federico Landi</strong>,
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=90">Marcella Cornia</a>,
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=122">Silvia Cascianelli</a>,	              	
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=82">Lorenzo Baraldi</a>,
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1">Rita Cucchiara</a>	
              <br>
              <em>ICPR</em>, 2020
		<font color="red">
		<strong>(Oral presentation)</strong>
		</font>
		<br>
		<a href="https://arxiv.org/abs/2007.07268" target="_blank">arXiv</a> &nbsp/&nbsp
		<a href="data/explore_and_explain.bib" target="_blank">bibtex</a> &nbsp/&nbsp
		<a href="data/Explore_and_Explain_Poster.pdf" target="_blank">poster</a> &nbsp/&nbsp
		<a href="data/Explore_and_Explain.pdf" target="_blank">slides</a> &nbsp/&nbsp
		<a href="data/Explore_and_Explain.mp4" target="_blank">presentation (video)</a>
		<br>
              <p></p>
              <p>A novel setting for Embodied AI in which an agent needs to explore a previously unknown environment while describing what it sees. The proposed model employs a self-supervised exploration module with penalty, and a fully-attentive captioning model for explanation.</p>
            </td>
        </tr>
        </tbody></table>
<!-- next paper -->
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="two" id='dynamic_filters_image'><img src='images/dynamic_filters.svg'></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://bmvc2019.org/wp-content/uploads/papers/0384-paper.pdf">
                <papertitle>Embodied Vision-and-Language Navigation with Dynamic Convolutional Filters</papertitle></a>
              <br>
			<strong>Federico Landi</strong>,              	
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=82">Lorenzo Baraldi</a>,
			<a href="http://vcg.isti.cnr.it/~corsini/">Massimiliano Corsini</a>,
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1">Rita Cucchiara</a>	
              <br>
              <em>BMVC</em>, 2019
		<font color="red">
		<strong>(Oral presentation)</strong>
		</font>
		<br>
		<a href="https://arxiv.org/abs/1907.02985" target="_blank">arXiv</a> &nbsp/&nbsp
		<a href="data/dynamic_filters.bib" target="_blank">bibtex</a> &nbsp/&nbsp
		<a href="https://github.com/aimagelab/DynamicConv-agent" target="_blank">code</a> &nbsp/&nbsp
		<a href="data/VLN_with_dynamic_convolutional_filters_poster.pdf" target="_blank">poster</a> &nbsp/&nbsp
		<a href="data/VLN_with_dynamic_convolutional_filters.pdf" target="_blank">slides</a> &nbsp/&nbsp
		<a href="https://www.youtube.com/watch?v=WHadUSUhL-o&t=16m50s" target="_blank">talk (video)</a>
		<br>
              <p></p>
              <p>In Vision-and-Language Navigation, an agent needs to reach a target destination with the only guidance of a natural language instruction.
We exploit dynamic convolutional filters to ground the lingual description into the visual observation in an elegant and efficient way.</p>
            </td>
        </tr>
        </tbody></table>
<!-- next paper -->
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="two" id='ucfcrime2local_image'><img src='images/anomaly_locality.svg'></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Anomaly locality in video surveillance</papertitle>
              <br>
			<strong>Federico Landi</strong>,
			<a href="https://www.ceessnoek.info/">Cees Snoek</a>,
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1">Rita Cucchiara</a>	
              <br>
              <em>ArXiv</em>, 2019
		<br>
		<a href="https://arxiv.org/abs/1901.10364" target="_blank">arXiv</a> &nbsp/&nbsp
		<a href="data/anomaly_locality.bib" target="_blank">bibtex</a> &nbsp/&nbsp
		<a href="https://drive.google.com/drive/folders/1Hu2oke7acBqcKKyUpv5NBmQ2fDiYn87c" target="_blank">dataset</a>
		<br>
              <p></p>
              <p>We explore the impact of considering spatiotemporal tubes instead of whole-frame video segments for anomaly detection in video surveillance. We create UCFCrime2Local: the first dataset for anomaly detection with bounding box supervision in both its train and test set.</p>
            </td>
        </tr>
        </tbody></table>
<!-- New paper template
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="two" id='id_image'><img src='images/image.png'></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Paper Title</papertitle>
              <br>
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=121">Roberto Bigazzi</a>,
			<strong>Federico Landi</strong>,
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=90">Marcella Cornia</a>,
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=122">Silvia Cascianelli</a>,	              	
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=82">Lorenzo Baraldi</a>,
			<a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1">Rita Cucchiara</a>	
              <br>
              <em>venue</em>, 2021
		<br>
		<a href="https://arxiv.org/abs/">arXiv</a> &nbsp/&nbsp
		<a href="data/.bib" target="_blank">bibtex</a> &nbsp/&nbsp
		<a href="https://github.com/">code</a>
		<br>
              <p></p>
              <p>Description</p>
            </td>
        </tr>
        </tbody></table> -->
<!-- end paper list -->
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Reviewing Service</heading>
			<p>
			  Journals:
            </p>
			<ul>
				<li><p>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</p></li>
				<li><p>IEEE Robotics and Automation Letters (RAL)</p></li>
				<li><p>ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</p></li>		
				<li><p>Pattern Recognition Letters (PRL)</p></li>			
			</ul>
			<p>
			  Conferences:
            </p>
			<ul>
				<li><p>ACM International Conference on Multimedia (ACMMM)</p></li>
				<li><p>IEEE International Conference on Robotics and Automation (ICRA)</p></li>
				<li><p>IEEE International Conference on Pattern Recognition (ICPR)</p></li>
			</ul>
			</td>
			
		</tr>
        </tbody></table>
	 <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching Activities</heading>
			<ul>
				<li><p>Computer Architecture - <em>Prof. Rita Cucchiara, Prof. Simone Calderara, 2020-2021</em></p></li>
				<li><p>Machine Learning and Deep Learning - <em>IFOA, 2020</em></p></li>
				<li><p>Deep Learning - <em>Nuova Didactica, 2020</em></p></li>		
			</ul>
			</td>
			
		</tr>
        </tbody></table>
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Courses and Summer Schools</heading>
			<ul>
			<li><p>Advanced Course on Data Science and Machine Learning - <em>ACDL 2020, Remote (<a href="https://fdlandi.github.io/data/ACDL-2020-Landi.pdf">certificate</a>)</em></p></li>
			<li><p>International Computer Vision Summer School - <em>ICVSS 2019, Scicli (RG), Italy (<a href="https://fdlandi.github.io/data/Federico_Landi_ICVSS2019_Exam_Certificate.pdf">certificate</a>)</em></p></li>
			</ul>
			</td>
			
		</tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Other</heading>
            		<ul>
				<li><p>In 2019, I carried on the 3D acquisition of the museum in Galleria Estense, in Modena (see below). One year later, the virtual spaces created for research purpose allowed to offer free guided tours to schools and young students during the Covid-19 lockdown in Italy.</p></li>
				<div class="three" id="gallerie_gif"><img style="margin-left:auto;margin-right:auto;width:80%;height:80%" src=data/Gallerie-Estensi.gif></div>
				<li><p>I like practicing <a href="https://en.wikipedia.org/wiki/Shuai_jiao" target="_blank">Shuai Jiao (摔跤)</a> and lifting weights.</p></li>
			</ul>
		</td>
		</tr>
        </tbody></table>
		
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">I like this website.</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>

